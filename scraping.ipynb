{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "search_url = \"https://tribunadonorte.com.br/?s=paulinho+freire\"\n",
    "base_url = \"https://tribunadonorte.com.br/\"\n",
    "\n",
    "\n",
    "def scrape_news_articles(search_url):\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    \n",
    "    news_data = []\n",
    "\n",
    "    \n",
    "    for h3 in soup.find_all('h3', class_=\"entry-title td-module-title\"):  \n",
    "        link = h3.find('a', href=True)\n",
    "        if link:\n",
    "            news_url = link['href']\n",
    "            \n",
    "            \n",
    "            if not news_url.startswith(\"http\"):\n",
    "                news_url = base_url + news_url\n",
    "\n",
    "            \n",
    "            news_response = requests.get(news_url)\n",
    "            news_soup = BeautifulSoup(news_response.text, 'html.parser')\n",
    "            \n",
    "            \n",
    "            title = news_soup.find('h1', class_=\"tdb-title-text\").text if news_soup.find('h1', class_=\"tdb-title-text\") else \"Sem título\"\n",
    "            \n",
    "            \n",
    "            paragraphs = news_soup.find_all('p')\n",
    "            content = \" \".join([p.get_text() for p in paragraphs]) if paragraphs else \"Sem conteúdo encontrado\"\n",
    "            \n",
    "            \n",
    "            news_data.append({\"Título\": title, \"Conteúdo\": content})\n",
    "            \n",
    "            \n",
    "            time.sleep(1)\n",
    "    \n",
    "    return news_data\n",
    "\n",
    "\n",
    "\n",
    "news_articles = scrape_news_articles(search_url)\n",
    "\n",
    "\n",
    "with open(\"noticias_paulinho_freire.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(news_articles, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Arquivo JSON criado com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "search_url = \"https://tribunadonorte.com.br/?s=Nat%C3%A1lia+Bonavides\"\n",
    "base_url = \"https://tribunadonorte.com.br/\"\n",
    "\n",
    "\n",
    "def scrape_news_articles(search_url):\n",
    "    response = requests.get(search_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    \n",
    "    news_data = []\n",
    "\n",
    "    \n",
    "    for h3 in soup.find_all('h3', class_=\"entry-title td-module-title\"):  \n",
    "        link = h3.find('a', href=True)\n",
    "        if link:\n",
    "            news_url = link['href']\n",
    "            \n",
    "            \n",
    "            if not news_url.startswith(\"http\"):\n",
    "                news_url = base_url + news_url\n",
    "\n",
    "            \n",
    "            news_response = requests.get(news_url)\n",
    "            news_soup = BeautifulSoup(news_response.text, 'html.parser')\n",
    "            \n",
    "            \n",
    "            title = news_soup.find('h1', class_=\"tdb-title-text\").text if news_soup.find('h1', class_=\"tdb-title-text\") else \"Sem título\"\n",
    "            \n",
    "            \n",
    "            paragraphs = news_soup.find_all('p')\n",
    "            content = \" \".join([p.get_text() for p in paragraphs]) if paragraphs else \"Sem conteúdo encontrado\"\n",
    "            \n",
    "            \n",
    "            news_data.append({\"Título\": title, \"Conteúdo\": content})\n",
    "            \n",
    "            \n",
    "            time.sleep(1)\n",
    "    \n",
    "    return news_data\n",
    "\n",
    "\n",
    "\n",
    "news_articles = scrape_news_articles(search_url)\n",
    "\n",
    "\n",
    "with open(\"noticias_natalia_bonavides.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(news_articles, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Arquivo JSON criado com sucesso!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
